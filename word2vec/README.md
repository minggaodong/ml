# word2vec 技术实践
## 背景
微博的用户关注关系，往往隐藏着一些重要的信息，比如一个用户关注列表中主要是汽车、理财、钓鱼等相关的大V账号，据此可以推得出一个中年男性的人群画像。再比如喜欢汽车的人往往会关注一批车评大V，这批被关注的大V可以被认定是相似账号，可以用来做账号推荐。

本次尝试将用户关注关系作为训练样本，利用 word2vec 来训练用户的 embeding 向量。

用户向量可以用于
- 作为点击率模型的用户侧特征使用
- 通过余弦计算进行相似账号的推荐


## word2vec 原理分析
### embedding
embeding 是将一类事物（比如用户，比如商品）嵌入到一个向量空间的过程，在这个向量空间中，不同事物个体之间的关系会通过向量的形式得到体现。

#### 和one-hot编码相比
- embeding 使用多维向量来表示一个特征，能够避免特征过多交叉产生的维度灾难。
- 向量包含了特征与特征之间在不同维度上的关系，在深度网络模型中很有价值。


### word2vec
word2vec 利用自然语言处理(NLP)的方式，将目标单词经过训练，得到embedding向量。

word2vec 本质上是一个单隐藏层的神经元网络，隐藏层的权重矩阵就是最终要产出的 embdedding 向量，而输出层产出的结果最终并不关心。

具体实现有 CBOW 和 Skip-gram 两种模型，CBOW 适合于数据集较小的情况，而 Skip-Gram 在大型语料中表现更好，下面都以Skip-Gram为例说明。

#### 模型描述

![image](https://user-images.githubusercontent.com/9280177/112304077-059e0a00-8cd8-11eb-9c0e-9c9abcc95600.png)

##### 输入层
中心词和周围词组成的单词对（feature，lable）。

##### 隐藏层
初始化一个w(v, n)矩阵，v是单词个数，n是embedding的维度。输入层的数据通过lookup得到词向量h(1, n).

##### 输出层
初始化一个w'(n, v)的矩阵，h(1, n)和w'(n, v)左乘，得到y(1, v)，再通过sofmax激活，得到当前中心词下，其他所有单词出现的概率。

##### 训练
取概率最大的单词为预测结果，和lable通过交叉熵得到损失函数，利用反向传播训练隐藏层的词向量。

##### 负采样优化
输出层每次训练都要对所有单词进行softmax激活运算，词汇量太大时效率很低，一般通过负采样进行效率上的优化。

负采样在输出层使用逻辑回归，将输入的上下文单词看成正例，随机采样的单词看成负例，这样激活函数就由softmax就变成了sigmod，每次也只需计算少量采样到的样本，效率提高很多。
训练


## 训练过程
### 生成词表
首先找到微博活跃粉丝数超过5万的账号，根据粉丝数排序，生成单词词表。word2vec模型将根据用户关注列表，训练这5万个账号，得到一个 W(50000, 128)的向量权重。

### 样本生成
将每个用户的关注列表作为样本，训练时将关注列表内的账号两两组合，生成输入样本。

使用1.2亿个用户的关注列表训练，每个关注列表平均长度100，产生1万条词对样本，所以整个样本训练集的规模在1万亿。

### 采样率控制
针对高频单词(这里指粉丝数最高的一批账号)，加入采样控制，通过采样率控制高频单词参与训练的次数，防止高频词汇训练过多，影响低频词效果

### 计算用户向量
通过word2vec训练的是5万个大v账号，只是得到了这5万个大v账号的用户向量，而项目的目标是得到所有活跃用户的向量。

基于用户和他的关注列表之间的关系，可以通过关注列表推导用户，这里将关注列表所有账号的向量求平均值，作为用户的向量。

用户向量通过文件保存，每一行代表一个用户向量，每个维度间用逗号隔开。
